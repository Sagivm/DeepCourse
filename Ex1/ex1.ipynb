{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNTn65kxBDiZiIHPKjhSHHz"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning - Assigment 1\n",
    "\n",
    "Sagiv Melamed - I.D.\n",
    "Dan Peled - I.D. 211547013"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "# from util import *\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Callable\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:40:35.907280Z",
     "end_time": "2023-04-04T00:41:14.306150Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1\n",
    "This section contains the implementations of the forward propagation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims: list) -> dict:\n",
    "    \"\"\"\n",
    "    Create an ANN architecture depending on layers_dims\n",
    "    :param layers_dims: list of layers dimentions\n",
    "    :type layers_dims: list\n",
    "    :return: dictionary built as follows:\n",
    "        W: list of matrices representing layer's weights, initialized randomly,\n",
    "        b: list of biases for each layer, initialized to zero\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    # Create W\n",
    "\n",
    "    W_sizes = [(next_dim, current_dim) for current_dim, next_dim in zip(layers_dims[:-1], layers_dims[1:])]\n",
    "    W = [np.random.randn(*Wi_size) for Wi_size in W_sizes]\n",
    "\n",
    "    # create b\n",
    "\n",
    "    b_sizes = layers_dims[1:]\n",
    "    b = [np.zeros((bi_size, 1)) for bi_size in b_sizes]\n",
    "\n",
    "    return {\n",
    "        \"W\": W,\n",
    "        \"b\": b\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.319427Z",
     "end_time": "2023-04-04T00:41:14.359179Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def linear_forward(A: np.ndarray, W: np.ndarray, B: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Performing linear forward on NN\n",
    "    :param A: Activation vector of previous layer\n",
    "    :type A: np.ndarray\n",
    "    :param W: Weight matrix of the current layer\n",
    "    :type W: np.ndarray\n",
    "    :param B: Bias vector of the current layer\n",
    "    :type B: np.ndarray\n",
    "    :return: dictionary built as follows:\n",
    "        Z: linear component of activation function\n",
    "        linear_cache: A,W,B\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"Z\": W.dot(A) + B,\n",
    "        \"linear_cache\": {\n",
    "            \"A\": A,\n",
    "            \"W\": W,\n",
    "            \"B\": B\n",
    "        }\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.345508Z",
     "end_time": "2023-04-04T00:41:14.366850Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Activation functions:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def softmax(Z: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Applying softmax on Z\n",
    "    :param Z: the linear component of the activation function\n",
    "    :type Z: np.ndarray\n",
    "    :return: dictionary built as follows:\n",
    "        A: Activation of th layer\n",
    "        activation_cache: Z\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    # To avoid overflow in the exponent we will subtract the max value from z\n",
    "    # and perform softmax on that. The mathematical proof provided in the report.\n",
    "    z = np.copy(Z)\n",
    "    z -= z.max(axis=0)\n",
    "    return {\n",
    "        \"A\": np.exp(z) / np.exp(z).sum(),\n",
    "        \"activation_cache\": {\n",
    "            \"Z\": Z\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def relu(Z: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "        Applying relu on Z\n",
    "        :param Z: the linear component of the activation function\n",
    "        :type Z: np.ndarray\n",
    "        :return: dictionary built as follows:\n",
    "            A: Activation of th layer\n",
    "            activation_cache: Z\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "    return {\n",
    "        \"A\": np.maximum(0, Z),\n",
    "        \"activation_cache\": {\n",
    "            \"Z\": Z\n",
    "        }\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.378643Z",
     "end_time": "2023-04-04T00:41:14.421213Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev: np.ndarray, W: np.ndarray, B: np.ndarray,\n",
    "                              activation: Callable[[np.ndarray], dict]) -> dict:\n",
    "    cache = {}\n",
    "    linear = linear_forward(A_prev, W, B)\n",
    "    z, linear_cache = linear['Z'], linear['linear_cache']\n",
    "\n",
    "    active = activation(z)\n",
    "    a, activation_cache = active['A'], active['activation_cache']\n",
    "\n",
    "    cache['linear_cache'] = linear_cache\n",
    "    cache['activation_cache'] = activation_cache\n",
    "\n",
    "    return {\n",
    "        \"A\": a,\n",
    "        \"cache\": cache\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.407009Z",
     "end_time": "2023-04-04T00:41:14.433885Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def L_model_forward(X: np.ndarray, parameters: dict, use_batchnorm: bool = False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param X: matrix of inputs\n",
    "    :type X: np.ndarray\n",
    "    :param parameters: a dict like object containing W and b\n",
    "    :type parameters: dict\n",
    "    :param use_batchnorm: whether to use batch normalization or not\n",
    "    :type use_batchnorm: bool\n",
    "    :return:\n",
    "        dictionary containing the activation of the ANN represented by the parameters on X and cache actions\n",
    "    :rtype:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    cache_list = list()\n",
    "    a = X\n",
    "\n",
    "    # Relu layers\n",
    "    for W_i, b_i in zip(parameters[\"W\"][:-1], parameters[\"b\"][:-1]):\n",
    "        results = linear_activation_forward(a, W_i, b_i, relu)\n",
    "        a = results['A']\n",
    "        if use_batchnorm:\n",
    "            a = apply_batchnorm(a)\n",
    "\n",
    "        cache_list.append(results['cache'])\n",
    "\n",
    "    # Softmax layer\n",
    "    results = linear_activation_forward(a, parameters[\"W\"][-1], parameters[\"b\"][-1], softmax)\n",
    "    cache_list.append(results['cache'])\n",
    "    return results['A'], cache_list\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.450199Z",
     "end_time": "2023-04-04T00:41:14.511327Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def compute_cost(Al: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute loss(cost) using prediction(Al) and true values(Y)\n",
    "    :param Al:\n",
    "    :type Al:\n",
    "    :param Y:\n",
    "    :type Y:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    return np.sum(Y * np.log(Al)) / -Y.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.470183Z",
     "end_time": "2023-04-04T00:41:14.511327Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def apply_batchnorm(A: np.ndarray) -> np.ndarray:\n",
    "    mean = A.mean()\n",
    "    std = A.std()\n",
    "\n",
    "    return (A - mean) / np.sqrt(std ** 2 + .0001)  # plus .0001 to avoid zero division"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.508783Z",
     "end_time": "2023-04-04T00:41:14.526253Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2\n",
    "\n",
    "This part contains functions related to the back propagation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def linear_backward(dZ: np.ndarray, cache: dict):\n",
    "    \"\"\"\n",
    "Implements the linear part of the backward propagation process for a single layer\n",
    "    :param dZ: the gradient of the cost with respect to the linear output of the current laye\n",
    "    :type dZ: np.ndarray\n",
    "    :param cache:\n",
    "    :type cache: dict\n",
    "    :return:\n",
    "        tuple of derivatives dA,dW,dB\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    dA = cache[\"W\"].T.dot(dZ)\n",
    "    dW = dZ.dot(cache['A'].T)\n",
    "    dB = np.sum(dZ, axis=1, keepdims=True)\n",
    "    return dA, dW, dB"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.543075Z",
     "end_time": "2023-04-04T00:41:14.588781Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA: np.ndarray, cache: dict, activation):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward function.\n",
    "    :param dA: post activation gradient of the current layer\n",
    "    :type dA: np.ndarray\n",
    "    :param cache: contains both the linear cache and the activations cache\n",
    "    :type cache: dict\n",
    "    :param activation: activation backward function\n",
    "    :type activation: function\n",
    "    :return:\n",
    "                tuple of derivatives dA,dW,dB\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    dZ = activation(dA, cache['activation_cache'])\n",
    "    return linear_backward(dZ, cache['linear_cache'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.598889Z",
     "end_time": "2023-04-04T00:41:14.672874Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def relu_backward(dA: np.ndarray, activation_catch: dict):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for a ReLU unit\n",
    "    :param dA: the post-activation gradient\n",
    "    :type dA: np.ndarray\n",
    "    :param activation_catch: contains Z (stored during the forward propagation)\n",
    "    :type activation_catch: dict\n",
    "    :return:\n",
    "        derivative of Z\n",
    "    :rtype:\n",
    "        np.ndarray\n",
    "    \"\"\"\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[activation_catch['Z'] <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def softmax_backward(dA, activation_cache):\n",
    "    return dA - activation_cache['Y']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.623488Z",
     "end_time": "2023-04-04T00:41:14.690706Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def l_model_backward(Al: np.ndarray, Y: np.ndarray, caches: list):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation process for the entire network.\n",
    "    :param Al: the probabilities vector, the output of the forward propagation\n",
    "    :type Al: np.ndarray\n",
    "    :param Y: the true labels vector (the \"ground truth\" - true classifications)\n",
    "    :type Y: np.ndarray\n",
    "    :param caches: contains Z (stored during the forward propagation)\n",
    "    :type caches: dict\n",
    "    :return:\n",
    "    gradient of the cost with respect to Z\n",
    "    :rtype:\n",
    "    np.ndarray\n",
    "    \"\"\"\n",
    "    layers = len(caches) - 1\n",
    "    grads = dict()\n",
    "\n",
    "    ## compute the gradient on predictions\n",
    "\n",
    "    # softmax layer update\n",
    "    layer = caches.pop()\n",
    "    layer['activation_cache']['Y'] = Y\n",
    "    grads[f\"dA_{layers}\"], grads[f\"dW_{layers}\"], grads[f\"dB_{layers}\"] = \\\n",
    "            linear_activation_backward(Al, layer, softmax_backward)\n",
    "    # Layers update\n",
    "    for i, layer in enumerate(reversed(caches)):\n",
    "        grads[f\"dA_{layers - i - 1}\"], grads[f\"dW_{layers - i - 1}\"], grads[f\"dB_{layers - i - 1}\"] = \\\n",
    "            linear_activation_backward(grads[f\"dA_{layers - i}\"], layer, relu_backward)\n",
    "        # dA = grads[f\"dA_{layers - i}\"]\n",
    "    return grads\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.665745Z",
     "end_time": "2023-04-04T00:41:14.695266Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def update_parameters(parameters: dict, grads: dict, learning_rate: float):\n",
    "    \"\"\"\n",
    "    Updates parameters using gradient descent\n",
    "    :param parameters: parameters of the ANN\n",
    "    :type parameters: dict\n",
    "    :param grads: – a python dictionary containing the gradients (generated by L_model_backward)\n",
    "    :type grads: dict\n",
    "    :param learning_rate: the learning rate used to update\n",
    "    :type learning_rate: float\n",
    "    :return:\n",
    "        Updated parameters of the ANN\n",
    "    :rtype:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    for index in range(len(parameters[\"W\"])):\n",
    "        parameters['W'][index] -= learning_rate * grads[f'dW_{index}']\n",
    "        parameters['b'][index] -= learning_rate * grads[f'dB_{index}']\n",
    "    return parameters\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.704399Z",
     "end_time": "2023-04-04T00:41:14.769305Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3\n",
    "\n",
    "This part contains the functions of training and testing a model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate, num_iterations, batch_size, ckpt=None):\n",
    "    \"\"\"\n",
    "    Train a model for one epoch.\n",
    "\n",
    "    :param X: The training data\n",
    "    :param Y: The training data's labels\n",
    "    :param layer_dims: iterable of integers represents the number of neurons in every layer.\n",
    "    :param learning_rate: the learning rate of the model\n",
    "    :param num_iterations: number of iteration to perform during 1 epoch\n",
    "    :param batch_size: size of 1 individual batch to feed the model at once.\n",
    "    :param ckpt: optional, pre-trained parameters for further training. If None, initialize new parameters.\n",
    "    :return: tuple of (parameters, costs). parameters are the trained model, and costs are the loss for every 100 iterations.\n",
    "    \"\"\"\n",
    "    params = ckpt if ckpt is not None else initialize_parameters(layer_dims)\n",
    "    batch_mask = np.zeros(X.shape[1])\n",
    "    batch_mask[:batch_size] = 1\n",
    "    costs = []\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        np.random.shuffle(batch_mask)\n",
    "        x = X[:, batch_mask.astype(bool)]\n",
    "        al, caches = L_model_forward(x, params)\n",
    "        y = Y[:, batch_mask.astype(bool)]\n",
    "        if i % 100 == 0:\n",
    "            costs.append(compute_cost(al, y))\n",
    "        grads = l_model_backward(al, y, caches)\n",
    "        params = update_parameters(params, grads, learning_rate)\n",
    "    return params, costs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.745315Z",
     "end_time": "2023-04-04T00:41:14.791147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters, use_batchnorm: bool=False):\n",
    "    predicted, _ = L_model_forward(X, parameters, use_batchnorm)\n",
    "    diff = np.argmax(predicted, axis=0) == np.argmax(Y, axis=0)\n",
    "    return diff.sum() / len(diff)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.801807Z",
     "end_time": "2023-04-04T00:41:14.856290Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 4\n",
    "Training the model over MNIST dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def _to_matrix(y):\n",
    "    \"\"\"\n",
    "    This function takes the y vector from the MNIST dataset and transform it to 1/0 matrix\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.eye(10)[y].T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:41:14.840967Z",
     "end_time": "2023-04-04T00:41:14.874546Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size = 48000\n",
      "Validation set size = 12000\n",
      "Test set size = 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = np.moveaxis(x_train, 0, -1).reshape((784, -1))\n",
    "x_test = np.moveaxis(x_test, 0, -1).reshape((784, -1))\n",
    "\n",
    "y_train = _to_matrix(y_train)\n",
    "y_test = _to_matrix(y_test)\n",
    "\n",
    "valdition_count = int(0.2 * y_train.shape[1])\n",
    "val_mask = np.zeros(y_train.shape[1])\n",
    "val_mask[:valdition_count] = 1\n",
    "np.random.shuffle(val_mask)\n",
    "val_mask = val_mask.astype(bool)\n",
    "x_val, y_val = x_train[:, val_mask], y_train[:, val_mask]\n",
    "x_train, y_train = x_train[:, ~val_mask], y_train[:, ~val_mask]\n",
    "\n",
    "print(f\"Train set size = {x_train.shape[1]}\\nValidation set size = {x_val.shape[1]}\\nTest set size = {x_test.shape[1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:42:45.954636Z",
     "end_time": "2023-04-04T00:42:47.479436Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': [array([[ 0.22798101,  0.52787335,  0.8497621 , ...,  0.39710117,\n",
      "        -0.6736147 , -0.50238817],\n",
      "       [-0.10572573,  1.2034748 ,  0.23126104, ...,  0.57854146,\n",
      "         1.91213453, -0.0121829 ],\n",
      "       [-1.21930458, -2.06363699, -0.61587093, ...,  0.57028286,\n",
      "        -0.99211108,  1.42395384],\n",
      "       ...,\n",
      "       [-0.52891753, -0.84789733,  1.18593307, ...,  0.25205403,\n",
      "         0.97201266, -0.13648241],\n",
      "       [ 1.66975654,  0.36441743,  0.01132962, ..., -0.73811998,\n",
      "         0.32376731,  0.83642622],\n",
      "       [ 0.54576875, -0.5449489 , -0.93497351, ..., -1.61328202,\n",
      "        -1.82176762,  1.43355309]]), array([[-5.08444848e+01,  7.70808931e+04,  7.73047537e+19,\n",
      "         2.28099438e+00, -1.06234734e+01, -3.81135278e+65,\n",
      "        -8.71267339e+61, -5.38182281e+01, -3.91629853e+65,\n",
      "        -4.34364770e+63, -4.43855972e+01, -3.27464360e+01,\n",
      "        -4.41611113e+43, -5.41663311e+43, -1.67469996e+47,\n",
      "        -4.78424823e+01,  7.19476323e-01, -2.99832326e+02,\n",
      "         1.26649694e+01, -6.18507850e+06],\n",
      "       [-3.89711357e+00,  5.63943213e+03,  2.19022725e+20,\n",
      "        -8.77061958e-01, -9.44240551e+00, -3.23546831e+64,\n",
      "        -7.39621343e+60, -1.66819346e+01, -3.32455705e+64,\n",
      "        -3.68733499e+62,  2.80538405e+01,  3.93527064e+01,\n",
      "        -3.74884940e+42, -4.59819538e+42, -1.42165760e+46,\n",
      "         1.09649848e+00,  1.54245173e-02,  1.10746693e+02,\n",
      "         7.39770866e+01, -3.60677219e+05],\n",
      "       [-4.27591709e+01, -9.36877152e-01, -2.12615712e+01,\n",
      "         1.37066140e+00, -5.84458578e+01, -5.27723092e+01,\n",
      "         9.24356736e-01, -1.24472172e+02,  2.45648148e-01,\n",
      "        -8.59399752e-01, -8.83195947e+01, -4.36483461e+01,\n",
      "         6.54577745e-01, -5.04192579e+01, -1.10952876e-02,\n",
      "        -9.01439556e+01,  1.56649635e+00, -3.44663179e+02,\n",
      "        -4.10159338e-01, -1.67754132e+01],\n",
      "       [-2.64282355e+01, -2.57109166e+04, -4.16167145e+04,\n",
      "         6.04925755e-02, -2.51496775e+00, -1.05332947e+07,\n",
      "        -1.13275402e+03, -1.85292982e+01, -9.97029447e+06,\n",
      "        -7.60162143e+05, -9.16900787e+00, -1.20806870e+01,\n",
      "        -1.46445942e+00, -4.91278709e-01, -2.70516278e+04,\n",
      "        -1.13194636e+01,  1.01424492e+00, -1.21392541e+02,\n",
      "         5.95226816e+00, -9.35363493e+05],\n",
      "       [-1.27537050e+00,  1.83719641e+00, -1.46324996e+05,\n",
      "         2.51803602e+00, -6.90343684e+00, -1.39123622e+07,\n",
      "         1.14808355e+00, -1.78431179e+01, -1.21632238e+07,\n",
      "        -1.75880736e+06, -2.05335079e+01, -8.56827691e+00,\n",
      "        -5.38810640e-01, -9.28886114e+00, -5.12491704e+04,\n",
      "        -1.97635085e+01, -4.27733012e-01, -6.54486110e+01,\n",
      "         5.22232587e-01, -7.33550874e+06],\n",
      "       [-8.09035378e-01,  3.10344854e-01, -3.28988417e+13,\n",
      "        -5.87722302e-01, -2.90215750e-01, -1.14122561e+12,\n",
      "        -4.82049875e+11,  1.72890031e+00, -3.76393717e+13,\n",
      "        -2.61576232e+13, -3.17621726e-01, -1.74436153e-01,\n",
      "        -5.81328645e-01, -8.68382234e-01, -2.28018993e+00,\n",
      "        -4.46241652e-02, -1.55711480e+00, -1.68601666e+00,\n",
      "         1.67164733e+00, -1.71366663e+00],\n",
      "       [-3.52014142e+01,  4.94849264e-02, -9.29252851e+00,\n",
      "         1.37862677e+00, -2.91835651e+00,  8.13896009e+00,\n",
      "        -2.74969610e-01, -3.57780554e+01, -2.95293743e+01,\n",
      "        -1.31462881e+01, -4.79936338e+01, -4.29003160e+01,\n",
      "        -8.14990484e-01, -8.98598427e+00,  4.19614145e-01,\n",
      "        -3.70296803e+01, -1.93630971e+00, -2.78139113e+02,\n",
      "        -2.83042374e+01, -1.90631529e+01]]), array([[-4.40232927e+62, -1.04948703e+63, -6.06754349e-01,\n",
      "         1.70612320e+04, -7.19674750e+03, -9.03426378e+11,\n",
      "         2.62143824e-01],\n",
      "       [-1.39154472e+63, -3.31735327e+63, -6.40013484e-01,\n",
      "         9.61098930e+03,  2.48966088e+03, -3.92067229e+11,\n",
      "         3.55912525e-01],\n",
      "       [-3.04793451e+02,  2.49218426e-01, -2.89429998e+01,\n",
      "        -5.53888353e+01, -8.67322059e+01, -8.08706217e-02,\n",
      "        -2.66385215e+02],\n",
      "       [-6.31320636e+64, -1.50502786e+65, -3.23300966e-01,\n",
      "         1.02770378e+06, -2.23841932e+05,  1.66866120e+12,\n",
      "         7.00829206e+01],\n",
      "       [-1.94670743e+64, -4.64082552e+64,  1.68880203e+01,\n",
      "         4.13738142e+05, -1.24053456e+05, -6.86092581e+13,\n",
      "         1.59756846e+02]]), array([[-9.22482195e-02, -4.02862780e-01, -1.66494824e+00,\n",
      "        -2.46038711e-02,  4.97662836e-01],\n",
      "       [ 4.57320564e+63,  2.30552537e+63,  2.20124174e+02,\n",
      "         3.03400397e+64,  1.99088812e+65],\n",
      "       [ 1.43918656e+64,  7.25548202e+63, -3.22775970e+01,\n",
      "         9.54800220e+64,  6.26531946e+65],\n",
      "       [ 5.56840364e+63,  2.80724220e+63,  2.33147338e-01,\n",
      "         3.69424865e+64,  2.42413517e+65],\n",
      "       [ 1.36357373e+19,  4.31648955e+19,  1.27816438e+02,\n",
      "         1.95747383e+21,  6.03458044e+20],\n",
      "       [-8.13571093e+63, -4.10151859e+63,  3.25362614e+02,\n",
      "        -5.39747854e+64, -3.54178043e+65],\n",
      "       [ 1.11994168e+00, -7.58317393e-01, -1.09843083e+00,\n",
      "        -7.23539050e-01, -1.42569876e+00],\n",
      "       [ 4.09983250e-01, -1.21121012e-01,  1.51926520e-01,\n",
      "         4.68056270e-01, -1.84554182e+00],\n",
      "       [-1.29875845e+05, -6.59194669e+04,  1.29446534e+02,\n",
      "        -9.15106876e+05, -5.44778559e+06],\n",
      "       [ 8.00936882e+63,  4.03782477e+63, -1.67357848e+02,\n",
      "         5.31365933e+64,  3.48677896e+65]])], 'b': [array([[-1.51110925e-02],\n",
      "       [-1.27172397e+02],\n",
      "       [-5.59957122e+15],\n",
      "       [-1.40750267e-02],\n",
      "       [-1.05818957e-01],\n",
      "       [-1.03517369e+61],\n",
      "       [-6.07138177e+59],\n",
      "       [-5.16432753e-02],\n",
      "       [-1.50435290e+62],\n",
      "       [-1.65258114e+62],\n",
      "       [-6.69010820e-02],\n",
      "       [-1.38583492e-01],\n",
      "       [ 2.58895810e+40],\n",
      "       [ 8.43835492e+41],\n",
      "       [-9.83868465e+44],\n",
      "       [-1.84430940e-01],\n",
      "       [ 0.00000000e+00],\n",
      "       [-2.76351483e-02],\n",
      "       [-4.87216718e-03],\n",
      "       [-2.42369735e+02]]), array([[-1.35441117e+41],\n",
      "       [-1.14976353e+40],\n",
      "       [-5.88517582e-02],\n",
      "       [-1.02536982e+02],\n",
      "       [-7.36830901e+01],\n",
      "       [-7.35026221e+03],\n",
      "       [-4.09660613e-02]]), array([[-1.52727060e+17],\n",
      "       [-4.82756152e+17],\n",
      "       [-3.17086854e-02],\n",
      "       [-2.19018835e+19],\n",
      "       [-6.75355757e+18]]), array([[0.31019872],\n",
      "       [0.36732269],\n",
      "       [0.2898472 ],\n",
      "       [0.2227038 ],\n",
      "       [0.34206667],\n",
      "       [0.261285  ],\n",
      "       [0.15291694],\n",
      "       [0.27390282],\n",
      "       [0.26415063],\n",
      "       [0.21560553]])]}\n"
     ]
    }
   ],
   "source": [
    "learning_rate = .009\n",
    "layers = [784, 20, 7, 5, 10]\n",
    "epoch = 0\n",
    "last_acc = 0  # used for the stopping criterion\n",
    "batch_size = 256\n",
    "\n",
    "costs = []\n",
    "\n",
    "params, c = L_layer_model(x_train, y_train, layers, learning_rate, 100, 4)\n",
    "print(params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:42:50.583871Z",
     "end_time": "2023-04-04T00:42:52.318298Z"
    }
   }
  }
 ]
}
