{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNTn65kxBDiZiIHPKjhSHHz"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning - Assigment 1\n",
    "\n",
    "Sagiv Melamed - I.D. 315092239 \\\n",
    "Dan Peled - I.D. 211547013"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "# from util import *\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Callable\n",
    "import tensorflow as tf\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T01:43:15.184532Z",
     "end_time": "2023-04-05T01:43:15.341775Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1\n",
    "This section contains the implementations of the forward propagation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims: list) -> dict:\n",
    "    \"\"\"\n",
    "    Create an ANN architecture depending on layers_dims\n",
    "    :param layers_dims: list of layers dimentions\n",
    "    :type layers_dims: list\n",
    "    :return: dictionary built as follows:\n",
    "        W: list of matrices representing layer's weights, initialized randomly,\n",
    "        b: list of biases for each layer, initialized to zero\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    # Create W\n",
    "\n",
    "    W_sizes = [(next_dim, current_dim) for current_dim, next_dim in zip(layers_dims[:-1], layers_dims[1:])]\n",
    "    W = [np.random.randn(*Wi_size) * np.sqrt(2 / Wi_size[1]) for Wi_size in W_sizes]\n",
    "\n",
    "    # create b\n",
    "\n",
    "    b_sizes = layers_dims[1:]\n",
    "    b = [np.zeros((bi_size, 1)) for bi_size in b_sizes]\n",
    "\n",
    "    return {\n",
    "        \"W\": W,\n",
    "        \"b\": b\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:00.815038Z",
     "end_time": "2023-04-05T00:38:00.884307Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def linear_forward(A: np.ndarray, W: np.ndarray, B: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Performing linear forward on NN\n",
    "    :param A: Activation vector of previous layer\n",
    "    :type A: np.ndarray\n",
    "    :param W: Weight matrix of the current layer\n",
    "    :type W: np.ndarray\n",
    "    :param B: Bias vector of the current layer\n",
    "    :type B: np.ndarray\n",
    "    :return: dictionary built as follows:\n",
    "        Z: linear component of activation function\n",
    "        linear_cache: A,W,B\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"Z\": W.dot(A) + B,\n",
    "        \"linear_cache\": {\n",
    "            \"A\": A,\n",
    "            \"W\": W,\n",
    "            \"B\": B\n",
    "        }\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:00.837714Z",
     "end_time": "2023-04-05T00:38:00.885301Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Activation functions:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def softmax(Z: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Applying softmax on Z\n",
    "    :param Z: the linear component of the activation function\n",
    "    :type Z: np.ndarray\n",
    "    :return: dictionary built as follows:\n",
    "        A: Activation of th layer\n",
    "        activation_cache: Z\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    # To avoid overflow in the exponent we will subtract the max value from z\n",
    "    # and perform softmax on that. The mathematical proof provided in the report.\n",
    "    z = np.copy(Z)\n",
    "    z -= z.max(axis=0)\n",
    "    return {\n",
    "        \"A\": np.exp(z) / np.exp(z).sum(axis=0),\n",
    "        \"activation_cache\": {\n",
    "            \"Z\": Z\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def relu(Z: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "        Applying relu on Z\n",
    "        :param Z: the linear component of the activation function\n",
    "        :type Z: np.ndarray\n",
    "        :return: dictionary built as follows:\n",
    "            A: Activation of th layer\n",
    "            activation_cache: Z\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "    return {\n",
    "        \"A\": np.maximum(0, Z),\n",
    "        \"activation_cache\": {\n",
    "            \"Z\": Z\n",
    "        }\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:00.894022Z",
     "end_time": "2023-04-05T00:38:00.952593Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev: np.ndarray, W: np.ndarray, B: np.ndarray,\n",
    "                              activation: Callable[[np.ndarray], dict]) -> dict:\n",
    "    cache = {}\n",
    "    linear = linear_forward(A_prev, W, B)\n",
    "    z, linear_cache = linear['Z'], linear['linear_cache']\n",
    "\n",
    "    active = activation(z)\n",
    "    a, activation_cache = active['A'], active['activation_cache']\n",
    "\n",
    "    cache['linear_cache'] = linear_cache\n",
    "    cache['activation_cache'] = activation_cache\n",
    "\n",
    "    return {\n",
    "        \"A\": a,\n",
    "        \"cache\": cache\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:00.934833Z",
     "end_time": "2023-04-05T00:38:00.987803Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def L_model_forward(X: np.ndarray, parameters: dict, use_batchnorm: bool = False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param X: matrix of inputs\n",
    "    :type X: np.ndarray\n",
    "    :param parameters: a dict like object containing W and b\n",
    "    :type parameters: dict\n",
    "    :param use_batchnorm: whether to use batch normalization or not\n",
    "    :type use_batchnorm: bool\n",
    "    :return:\n",
    "        dictionary containing the activation of the ANN represented by the parameters on X and cache actions\n",
    "    :rtype:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    cache_list = list()\n",
    "    a = X\n",
    "\n",
    "    # Relu layers\n",
    "    for W_i, b_i in zip(parameters[\"W\"][:-1], parameters[\"b\"][:-1]):\n",
    "        results = linear_activation_forward(a, W_i, b_i, relu)\n",
    "        a = results['A']\n",
    "        if use_batchnorm:\n",
    "            a = apply_batchnorm(a)\n",
    "\n",
    "        cache_list.append(results['cache'])\n",
    "\n",
    "    # Softmax layer\n",
    "    results = linear_activation_forward(a, parameters[\"W\"][-1], parameters[\"b\"][-1], softmax)\n",
    "    cache_list.append(results['cache'])\n",
    "    return results['A'], cache_list\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:00.963204Z",
     "end_time": "2023-04-05T00:38:01.059596Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def compute_cost(Al: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute loss(cost) using prediction(Al) and true values(Y)\n",
    "    :param Al:\n",
    "    :type Al:\n",
    "    :param Y:\n",
    "    :type Y:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    return np.sum(Y * np.log(Al)) / -Y.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:00.998853Z",
     "end_time": "2023-04-05T00:38:01.061109Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def apply_batchnorm(A: np.ndarray) -> np.ndarray:\n",
    "    mean = A.mean()\n",
    "    std = A.std()\n",
    "\n",
    "    return (A - mean) / np.sqrt(std ** 2 + .0001)  # plus .0001 to avoid zero division"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.030769Z",
     "end_time": "2023-04-05T00:38:01.061642Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2\n",
    "\n",
    "This part contains functions related to the back propagation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def linear_backward(dZ: np.ndarray, cache: dict):\n",
    "    \"\"\"\n",
    "Implements the linear part of the backward propagation process for a single layer\n",
    "    :param dZ: the gradient of the cost with respect to the linear output of the current laye\n",
    "    :type dZ: np.ndarray\n",
    "    :param cache:\n",
    "    :type cache: dict\n",
    "    :return:\n",
    "        tuple of derivatives dA,dW,dB\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    m = cache[\"A\"].shape[1]\n",
    "\n",
    "    dA = cache[\"W\"].T.dot(dZ)\n",
    "    dW = dZ.dot(cache['A'].T) / m\n",
    "    dB = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    return dA, dW, dB"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.059596Z",
     "end_time": "2023-04-05T00:38:01.081055Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA: np.ndarray, cache: dict, activation):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward function.\n",
    "    :param dA: post activation gradient of the current layer\n",
    "    :type dA: np.ndarray\n",
    "    :param cache: contains both the linear cache and the activations cache\n",
    "    :type cache: dict\n",
    "    :param activation: activation backward function\n",
    "    :type activation:\n",
    "    :return:\n",
    "                tuple of derivatives dA,dW,dB\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    dZ = activation(dA, cache['activation_cache'])\n",
    "    return linear_backward(dZ, cache['linear_cache'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.096327Z",
     "end_time": "2023-04-05T00:38:01.200463Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def relu_backward(dA: np.ndarray, activation_catch: dict):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for a ReLU unit\n",
    "    :param dA: the post-activation gradient\n",
    "    :type dA: np.ndarray\n",
    "    :param activation_catch: contains Z (stored during the forward propagation)\n",
    "    :type activation_catch: dict\n",
    "    :return:\n",
    "        derivative of Z\n",
    "    :rtype:\n",
    "        np.ndarray\n",
    "    \"\"\"\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[activation_catch['Z'] <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def softmax_backward(dA, activation_cache):\n",
    "    return dA\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.122048Z",
     "end_time": "2023-04-05T00:38:01.200463Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def l_model_backward(Al: np.ndarray, Y: np.ndarray, caches: list):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation process for the entire network.\n",
    "    :param Al: the probabilities vector, the output of the forward propagation\n",
    "    :type Al: np.ndarray\n",
    "    :param Y: the true labels vector (the \"ground truth\" - true classifications)\n",
    "    :type Y: np.ndarray\n",
    "    :param caches: contains Z (stored during the forward propagation)\n",
    "    :type caches: dict\n",
    "    :return:\n",
    "    gradient of the cost with respect to Z\n",
    "    :rtype:\n",
    "    np.ndarray\n",
    "    \"\"\"\n",
    "    layers = len(caches)\n",
    "    grads = dict()\n",
    "\n",
    "    ## compute the gradient on predictions\n",
    "\n",
    "    # softmax layer update\n",
    "    current_cache = caches[layers - 1]\n",
    "    dA_last = Al - Y  # gradient of loss function and softmax\n",
    "    grads[f\"dA_{layers - 1}\"], grads[f\"dW_{layers - 1}\"], grads[f\"dB_{layers - 1}\"] = \\\n",
    "            linear_activation_backward(dA_last, current_cache, softmax_backward)\n",
    "    # Layers update\n",
    "    for l in reversed(range(layers - 1)):\n",
    "        current_cache = caches[l]\n",
    "        grads[f\"dA_{l}\"], grads[f\"dW_{l}\"], grads[f\"dB_{l}\"] = \\\n",
    "            linear_activation_backward(grads[f\"dA_{l + 1}\"], current_cache, relu_backward)\n",
    "        # dA = grads[f\"dA_{layers - i}\"]\n",
    "    return grads\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.160289Z",
     "end_time": "2023-04-05T00:38:01.201996Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def update_parameters(parameters: dict, grads: dict, learning_rate: float):\n",
    "    \"\"\"\n",
    "    Updates parameters using gradient descent\n",
    "    :param parameters: parameters of the ANN\n",
    "    :type parameters: dict\n",
    "    :param grads: â€“ a python dictionary containing the gradients (generated by L_model_backward)\n",
    "    :type grads: dict\n",
    "    :param learning_rate: the learning rate used to update\n",
    "    :type learning_rate: float\n",
    "    :return:\n",
    "        Updated parameters of the ANN\n",
    "    :rtype:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    for index in range(len(parameters[\"W\"])):\n",
    "        parameters['W'][index] -= learning_rate * grads[f'dW_{index}']\n",
    "        parameters['b'][index] -= learning_rate * grads[f'dB_{index}']\n",
    "    return parameters\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.180184Z",
     "end_time": "2023-04-05T00:38:01.260958Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3\n",
    "\n",
    "This part contains the functions of training and testing a model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def _build_mini_batches(X, Y, batch_size):\n",
    "    \"\"\"\n",
    "    Splits the data set to mini-batches in size of batch_size.\n",
    "    If the dataset length divides with a remaining by batch_size, another mini-batch will be appended in the size of the remaining.\n",
    "    :param X:\n",
    "    :param Y:\n",
    "    :param batch_size:\n",
    "    :return: list of mini-batches structured by (x,y)\n",
    "    \"\"\"\n",
    "\n",
    "    # Shuffle the dataset first\n",
    "    permutation = np.random.permutation(Y.shape[1])\n",
    "    x_shuffled, y_shuffled = X[:, permutation], Y[:, permutation]\n",
    "\n",
    "    batches_count, remain = divmod(Y.shape[1], batch_size)\n",
    "    mini_batches = []\n",
    "    for i in range(batches_count):\n",
    "        mini_x = x_shuffled[:, i * batch_size:(i + 1) * batch_size]\n",
    "        mini_y = y_shuffled[:, i * batch_size:(i + 1) * batch_size]\n",
    "        mini_batches.append((mini_x, mini_y))\n",
    "\n",
    "    if remain:\n",
    "        mini_x = x_shuffled[:, -remain:]\n",
    "        mini_y = y_shuffled[:, -remain:]\n",
    "        mini_batches.append((mini_x, mini_y))\n",
    "\n",
    "    return mini_batches"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.209065Z",
     "end_time": "2023-04-05T00:38:01.308728Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate, num_iterations, batch_size, stop_eps=1e-5):\n",
    "    \"\"\"\n",
    "    Train a model for one epoch.\n",
    "\n",
    "    :param X: The training data\n",
    "    :param Y: The training data's labels\n",
    "    :param layer_dims: iterable of integers represents the number of neurons in every layer.\n",
    "    :param learning_rate: the learning rate of the model\n",
    "    :param num_iterations: maximum number of epochs, if the stopping by the validation won't occur.\n",
    "    :param batch_size: size of 1 individual batch to feed the model at once.\n",
    "    :param stop_eps: optional, the bound of validation cose change for stop training. default 1e-5.\n",
    "    :return: tuple of (parameters, costs, metadata).\n",
    "        parameters are the trained model, costs are the loss for every 100 training update, and metadata about the training - runtime, number of epochs, accuracies of the train and validation, batch size,\n",
    "    \"\"\"\n",
    "\n",
    "    (x_train, y_train), (x_val, y_val) = _split_train_val(X, Y)\n",
    "    mini_batches = _build_mini_batches(x_train, y_train, batch_size)\n",
    "\n",
    "    params = initialize_parameters(layer_dims)\n",
    "\n",
    "    costs = []\n",
    "    last_val_cost = -np.inf\n",
    "\n",
    "    update_count = 0  # number of time called to update_parameters\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        print(f\"Start epoch {i}\")\n",
    "        # Train\n",
    "        for (x, y) in mini_batches:\n",
    "            al, caches = L_model_forward(x, params)\n",
    "            if (update_count + 1) % 100 == 0:\n",
    "                costs.append(compute_cost(al, y))\n",
    "            grads = l_model_backward(al, y, caches)\n",
    "            params = update_parameters(params, grads, learning_rate)\n",
    "            update_count += 1\n",
    "\n",
    "        # Validation\n",
    "        val_al, _ = L_model_forward(x_val, params)\n",
    "        val_cost = compute_cost(val_al, y_val)\n",
    "        print(f\"End epoch {i} - {val_cost=}\")\n",
    "        if i > 15 and np.abs(val_cost - last_val_cost) < stop_eps:\n",
    "            # The stopping checking is done only after 15 epochs to avoid fast stopping on high cost\n",
    "            print(f\"Stopping after the validation cost wasn't changed: {last_val_cost=}, {val_cost=}\"\n",
    "                  f\" diff={np.abs(val_cost - last_val_cost)}\")\n",
    "            break\n",
    "        last_val_cost = val_cost\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    train_acc = predict(x_train, y_train, params)\n",
    "    val_acc = predict(x_val, y_val, params)\n",
    "\n",
    "    return params, costs, {\"runtime\": end-start, \"epochs\": i, \"val_acc\": val_acc, \"train_acc\": train_acc, \"batch_size\": batch_size, \"mini-batches\": len(mini_batches)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.273637Z",
     "end_time": "2023-04-05T00:38:01.311245Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters, use_batchnorm: bool=False):\n",
    "    predicted, _ = L_model_forward(X, parameters, use_batchnorm)\n",
    "    diff = np.argmax(predicted, axis=0) == np.argmax(Y, axis=0)\n",
    "    return diff.sum() / len(diff)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.311245Z",
     "end_time": "2023-04-05T00:38:01.354767Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 4\n",
    "Training the model over MNIST dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def _to_matrix(y):\n",
    "    \"\"\"\n",
    "    This function takes the y vector from the MNIST dataset and transform it to one-hot matrix\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.eye(10)[y].T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.333930Z",
     "end_time": "2023-04-05T00:38:01.402407Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def _split_train_val(X, Y, ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits train set to train and validation\n",
    "    :param X:\n",
    "    :param Y:\n",
    "    :return: (x_train, y_train), (x_val, y_val)\n",
    "    \"\"\"\n",
    "    valdition_count = int(ratio * Y.shape[1])\n",
    "    val_mask = np.zeros(Y.shape[1])\n",
    "    val_mask[:valdition_count] = 1\n",
    "    np.random.shuffle(val_mask)\n",
    "    val_mask = val_mask.astype(bool)\n",
    "    x_val, y_val = X[:, val_mask], Y[:, val_mask]\n",
    "    x_train, y_train = X[:, ~val_mask], Y[:, ~val_mask]\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.354767Z",
     "end_time": "2023-04-05T00:38:01.440029Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = np.moveaxis(x_train, 0, -1).reshape((784, -1)) / 255.0\n",
    "x_test = np.moveaxis(x_test, 0, -1).reshape((784, -1)) / 255.0\n",
    "\n",
    "y_train = _to_matrix(y_train)\n",
    "y_test = _to_matrix(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:38:01.391738Z",
     "end_time": "2023-04-05T00:38:02.384069Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1\n",
      "End epoch 1 - val_cost=0.46255522330357857\n",
      "Start epoch 2\n",
      "End epoch 2 - val_cost=0.3411523715674415\n",
      "Start epoch 3\n",
      "End epoch 3 - val_cost=0.2966929967762058\n",
      "Start epoch 4\n",
      "End epoch 4 - val_cost=0.28171416594345267\n",
      "Start epoch 5\n",
      "End epoch 5 - val_cost=0.2574458456179455\n",
      "Start epoch 6\n",
      "End epoch 6 - val_cost=0.2490123689772437\n",
      "Start epoch 7\n",
      "End epoch 7 - val_cost=0.23942916159337554\n",
      "Start epoch 8\n",
      "End epoch 8 - val_cost=0.2322655953490141\n",
      "Start epoch 9\n",
      "End epoch 9 - val_cost=0.22443946040601703\n",
      "Start epoch 10\n",
      "End epoch 10 - val_cost=0.22294040319678746\n",
      "Start epoch 11\n",
      "End epoch 11 - val_cost=0.21881220560272333\n",
      "Start epoch 12\n",
      "End epoch 12 - val_cost=0.21654011975198198\n",
      "Start epoch 13\n",
      "End epoch 13 - val_cost=0.21312414726422252\n",
      "Start epoch 14\n",
      "End epoch 14 - val_cost=0.21201312329944538\n",
      "Start epoch 15\n",
      "End epoch 15 - val_cost=0.2137730146843351\n",
      "Start epoch 16\n",
      "End epoch 16 - val_cost=0.21404595887986133\n",
      "Start epoch 17\n",
      "End epoch 17 - val_cost=0.2148128118636058\n",
      "Start epoch 18\n",
      "End epoch 18 - val_cost=0.21627993661277875\n",
      "Start epoch 19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m layers \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m784\u001B[39m, \u001B[38;5;241m20\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m10\u001B[39m]\n\u001B[0;32m      3\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m\n\u001B[1;32m----> 5\u001B[0m params, c, metadata \u001B[38;5;241m=\u001B[39m \u001B[43mL_layer_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_eps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0001\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[15], line 29\u001B[0m, in \u001B[0;36mL_layer_model\u001B[1;34m(X, Y, layer_dims, learning_rate, num_iterations, batch_size, stop_eps)\u001B[0m\n\u001B[0;32m     26\u001B[0m     al, caches \u001B[38;5;241m=\u001B[39m L_model_forward(x, params)\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;66;03m# if i % 100 == 0:\u001B[39;00m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;66;03m#     costs.append(compute_cost(al, y))\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m     grads \u001B[38;5;241m=\u001B[39m \u001B[43ml_model_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaches\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m     params \u001B[38;5;241m=\u001B[39m update_parameters(params, grads, learning_rate)\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# Validation\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[12], line 29\u001B[0m, in \u001B[0;36ml_model_backward\u001B[1;34m(Al, Y, caches)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mrange\u001B[39m(layers \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[0;32m     27\u001B[0m     current_cache \u001B[38;5;241m=\u001B[39m caches[l]\n\u001B[0;32m     28\u001B[0m     grads[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdA_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00ml\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m], grads[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdW_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00ml\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m], grads[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdB_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00ml\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m---> 29\u001B[0m         \u001B[43mlinear_activation_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdA_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43ml\u001B[49m\u001B[38;5;250;43m \u001B[39;49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;250;43m \u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcurrent_cache\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrelu_backward\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;66;03m# dA = grads[f\"dA_{layers - i}\"]\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grads\n",
      "Cell \u001B[1;32mIn[10], line 15\u001B[0m, in \u001B[0;36mlinear_activation_backward\u001B[1;34m(dA, cache, activation)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mImplements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward function.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m:param dA: post activation gradient of the current layer\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m:rtype:\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     14\u001B[0m dZ \u001B[38;5;241m=\u001B[39m activation(dA, cache[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mactivation_cache\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlinear_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdZ\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlinear_cache\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[9], line 15\u001B[0m, in \u001B[0;36mlinear_backward\u001B[1;34m(dZ, cache)\u001B[0m\n\u001B[0;32m     12\u001B[0m m \u001B[38;5;241m=\u001B[39m cache[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     14\u001B[0m dA \u001B[38;5;241m=\u001B[39m cache[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mT\u001B[38;5;241m.\u001B[39mdot(dZ)\n\u001B[1;32m---> 15\u001B[0m dW \u001B[38;5;241m=\u001B[39m \u001B[43mdZ\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcache\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mA\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m m\n\u001B[0;32m     16\u001B[0m dB \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(dZ, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m/\u001B[39m m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dA, dW, dB\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = .009\n",
    "layers = [784, 20, 7, 5, 10]\n",
    "batch_size = 8\n",
    "\n",
    "params, c, metadata = L_layer_model(x_train, y_train, layers, learning_rate, 100, batch_size, stop_eps=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:41:30.192414Z",
     "end_time": "2023-04-05T00:43:00.798334Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final accuracy of the test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 93.27%\n"
     ]
    }
   ],
   "source": [
    "acc = predict(x_test, y_test, params) * 100\n",
    "print(f\"Test accuracy {acc:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T01:36:57.888850Z",
     "end_time": "2023-04-05T01:36:59.410066Z"
    }
   }
  }
 ]
}
