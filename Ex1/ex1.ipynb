{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNTn65kxBDiZiIHPKjhSHHz"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning - Assigment 1\n",
    "\n",
    "Sagiv Melamed - I.D.\n",
    "Dan Peled - I.D. 211547013"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "# from util import *\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Callable\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-28T00:27:52.318104Z",
     "end_time": "2023-03-28T00:27:52.338662Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1\n",
    "This section contains the implementations of the forward propagation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims: list) -> dict:\n",
    "    \"\"\"\n",
    "    Create an ANN architecture depending on layers_dims\n",
    "    :param layers_dims: list of layers dimentions\n",
    "    :type layers_dims: list\n",
    "    :return: dictionary built as follows:\n",
    "        W: list of matrices representing layer's weights, initialized randomly,\n",
    "        b: list of biases for each layer, initialized to zero\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    # Create W\n",
    "\n",
    "    W_sizes = [(next_dim, current_dim) for current_dim, next_dim in zip(layers_dims[:-1], layers_dims[1:])]\n",
    "    W = [np.random.randn(*Wi_size) for Wi_size in W_sizes]\n",
    "\n",
    "    # create b\n",
    "\n",
    "    b_sizes = layers_dims[1:]\n",
    "    b = [np.zeros((1, bi_size)) for bi_size in b_sizes]\n",
    "\n",
    "    return {\n",
    "        \"W\": W,\n",
    "        \"b\": b\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-28T00:27:55.071279Z",
     "end_time": "2023-03-28T00:27:55.183268Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def linear_forward(A: np.ndarray, W: np.ndarray, B: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Performing linear forward on NN\n",
    "    :param A: Activation vector of previous layer\n",
    "    :type A: np.ndarray\n",
    "    :param W: Weight matrix of the current layer\n",
    "    :type W: np.ndarray\n",
    "    :param B: Bias vector of the current layer\n",
    "    :type B: np.ndarray\n",
    "    :return: dictionary built as follows:\n",
    "        Z: linear component of activation function\n",
    "        linear_cache: A,W,B\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"Z\": A.dot(W.T) + B,\n",
    "        \"linear_cache\": {\n",
    "            \"A\": A,\n",
    "            \"W\": W,\n",
    "            \"B\": B\n",
    "        }\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-28T00:27:55.615554Z",
     "end_time": "2023-03-28T00:27:55.642720Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Activation functions:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def softmax(Z: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Applying softmax on Z\n",
    "    :param Z: the linear component of the activation function\n",
    "    :type Z: np.ndarray\n",
    "    :return: dictionary built as follows:\n",
    "        A: Activation of th layer\n",
    "        activation_cache: Z\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"A\": np.exp(Z) / np.exp(Z).sum(),\n",
    "        \"activation_cahce\": {\n",
    "            \"Z\": Z\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def relu(Z: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "        Applying relu on Z\n",
    "        :param Z: the linear component of the activation function\n",
    "        :type Z: np.ndarray\n",
    "        :return: dictionary built as follows:\n",
    "            A: Activation of th layer\n",
    "            activation_cache: Z\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "    return {\n",
    "        \"A\": np.maximum(0, Z),\n",
    "        \"activation_cahce\": {\n",
    "            \"Z\": Z\n",
    "        }\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-28T00:27:56.907258Z",
     "end_time": "2023-03-28T00:27:56.925670Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev: np.ndarray, W: np.ndarray, B: np.ndarray,\n",
    "                              activation: Callable[[np.ndarray], dict]) -> dict:\n",
    "    cache = {}\n",
    "    linear = linear_forward(A_prev, W, B)\n",
    "    z, linear_cache = linear['Z'], linear['linear_cache']\n",
    "\n",
    "    active = activation(z)\n",
    "    a, activation_cache = active['A'], active['activation_cache']\n",
    "\n",
    "    cache.update(linear_cache)\n",
    "    cache.update(activation_cache)\n",
    "\n",
    "    return {\n",
    "        \"A\": a,\n",
    "        \"cache\": cache\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-28T00:27:57.784280Z",
     "end_time": "2023-03-28T00:27:57.827449Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def L_model_forward(X: np.ndarray, parameters: dict, use_batchnorm: bool = False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param X: matrix of inputs\n",
    "    :type X: np.ndarray\n",
    "    :param parameters: a dict like object containing W and b\n",
    "    :type parameters: dict\n",
    "    :param use_batchnorm: whether to use batch normalization or not\n",
    "    :type use_batchnorm: bool\n",
    "    :return:\n",
    "        dictionary containing the activation of the ANN represented by the parameters on X and cache actions\n",
    "    :rtype:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    cache_list = list()\n",
    "    A = X\n",
    "\n",
    "    # Relu layers\n",
    "    for W_i, b_i in zip(parameters[\"W\"][:-1], parameters[\"b\"][:-1]):\n",
    "        results = linear_activation_forward(A, W_i, b_i, relu)\n",
    "        A = results['A']\n",
    "        if use_batchnorm:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        cache_list.append(results['cache'])\n",
    "\n",
    "    # Softmax layer\n",
    "    cache = {}\n",
    "    Z, cache[\"linear_cache\"] = list(linear_forward(A, parameters[\"W\"][-1], parameters[\"b\"][-1]).values())\n",
    "    y, cache[\"activation_cache\"] = list(softmax(Z).values())\n",
    "    cache_list.append(cache)\n",
    "    return y, cache_list\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-28T00:27:59.177721Z",
     "end_time": "2023-03-28T00:27:59.217240Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
